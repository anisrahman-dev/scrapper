name: Fetch latest news

on:
  schedule:
    - cron: "*/30 * * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4

      - name: Fetch news JSON (all sources)
        run: |
          python - <<'PY'
          import json
          import urllib.request
          from datetime import datetime, timezone
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlparse

          # ---- Prothom Alo API ----
          SLUGS = {
              "latest-all": "latest",
              "bangladesh-all": "bangladesh",
              "politics-all": "politics",
              "world-all": "world",
              "business-all": "business",
              "sports-all": "sports",
              "entertainment-all": "entertainment",
              "chakri-all": "chakri",
          }

          MEDIA_BASE = "https://media.prothomalo.com/"
          HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
              "Accept": "application/json",
          }

          def fetch_api(slug):
              url = f"https://www.prothomalo.com/api/v1/collections/{slug}"
              try:
                  req = urllib.request.Request(url, headers=HEADERS)
                  with urllib.request.urlopen(req, timeout=20) as resp:
                      return json.load(resp)
              except Exception as e:
                  print(f"ProthomAlo fetch failed for {slug}: {e}")
                  return {"items": []}

          def image_key(story):
              hero = story.get("hero-image-s3-key")
              if hero:
                  return hero
              for card in story.get("cards", []):
                  for el in card.get("story-elements", []):
                      if el.get("type") == "image" and el.get("image-s3-key"):
                          return el["image-s3-key"]
              return None

          def image_url_from_key(key):
              if not key:
                  return None
              return f"{MEDIA_BASE}{key}?auto=format,compress"

          def extract_stories_api(data):
              stories = []
              for item in data.get("items", []):
                  story = item.get("story")
                  if not story:
                      continue
                  headline = story.get("headline")
                  if isinstance(headline, list):
                      headline = headline[0] if headline else None
                  key = image_key(story)
                  stories.append({
                      "headline": headline,
                      "url": story.get("url"),
                      "image_s3_key": key,
                      "image_url": image_url_from_key(key),
                  })
              return stories

          # ---- Other sites HTML scrape (domain-filtered anchors, better image attrs) ----
          HTML_SOURCES = [
              # bd-pratidin
              {"name": "bd_pratidin_todaynews", "url": "https://www.bd-pratidin.com/online/todaynews", "domain": "www.bd-pratidin.com"},
              {"name": "bd_pratidin_national", "url": "https://www.bd-pratidin.com/national", "domain": "www.bd-pratidin.com"},
              # ittefaq
              {"name": "ittefaq_latest", "url": "https://www.ittefaq.com.bd/latest-news", "domain": "www.ittefaq.com.bd"},
              {"name": "ittefaq_country", "url": "https://www.ittefaq.com.bd/country", "domain": "www.ittefaq.com.bd"},
              {"name": "ittefaq_politics", "url": "https://www.ittefaq.com.bd/politics", "domain": "www.ittefaq.com.bd"},
              {"name": "ittefaq_sports", "url": "https://www.ittefaq.com.bd/sports", "domain": "www.ittefaq.com.bd"},
              {"name": "ittefaq_world", "url": "https://www.ittefaq.com.bd/world-news", "domain": "www.ittefaq.com.bd"},
              {"name": "ittefaq_tech", "url": "https://www.ittefaq.com.bd/tech", "domain": "www.ittefaq.com.bd"},
              # kalerkantho (may be blocked by Cloudflare)
              {"name": "kaler_recent", "url": "https://www.kalerkantho.com/special/recent", "domain": "www.kalerkantho.com"},
              {"name": "kaler_world", "url": "https://www.kalerkantho.com/online/world", "domain": "www.kalerkantho.com"},
              {"name": "kaler_sport", "url": "https://www.kalerkantho.com/online/sport", "domain": "www.kalerkantho.com"},
              # nayadiganta
              {"name": "naya_latest", "url": "https://dailynayadiganta.com/latest", "domain": "dailynayadiganta.com"},
              # amarsangbad
              {"name": "amar_bangladesh", "url": "https://www.amarsangbad.com/bangladesh", "domain": "www.amarsangbad.com"},
              {"name": "amar_politics", "url": "https://www.amarsangbad.com/politics", "domain": "www.amarsangbad.com"},
          ]

          HTML_HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
          }

          def fetch_html(url):
              req = urllib.request.Request(url, headers=HTML_HEADERS)
              with urllib.request.urlopen(req, timeout=20) as resp:
                  return resp.read()

          def first_src_from_img(img_tag):
              if not img_tag:
                  return None
              srcset = img_tag.get("data-srcset") or img_tag.get("srcset")
              if srcset:
                  first = srcset.split(",")[0].strip().split()[0]
                  if first:
                      return first
              for attr in ("data-src", "data-original", "src"):
                  val = img_tag.get(attr)
                  if val:
                      return val
              return None

          def same_domain(href, domain):
              if not href:
                  return False
              if href.startswith("/"):
                  return True
              netloc = urlparse(href).netloc
              return (netloc == "" or netloc == domain)

          def scrape_site(conf, max_items=30):
              html = fetch_html(conf["url"])
              soup = BeautifulSoup(html, "html.parser")
              items = []
              for a in soup.find_all("a"):
                  href = a.get("href")
                  text = a.get_text(strip=True)
                  if not href or not text or len(text) < 20:
                      continue
                  if not same_domain(href, conf["domain"]):
                      continue
                  full_href = urljoin(conf["url"], href)

                  img_url = None
                  parent = a.parent
                  img = parent.find("img") if parent else None
                  if not img:
                      img = a.find("img")
                  raw_img = first_src_from_img(img) if img else None
                  if raw_img:
                      img_url = urljoin(conf["url"], raw_img)

                  items.append({"headline": text, "url": full_href, "image_url": img_url})
                  if len(items) >= max_items:
                      break
              return items

          # ---- Run all fetches ----
          sections = {}

          for slug, name in SLUGS.items():
              sections[name] = extract_stories_api(fetch_api(slug))

          for conf in HTML_SOURCES:
              try:
                  sections[conf["name"]] = scrape_site(conf)
              except Exception as e:
                  print(f"HTML scrape failed for {conf['name']}: {e}")
                  sections[conf["name"]] = []

          fetched_at = datetime.now(timezone.utc).isoformat()

          with open("news_sections.json", "w", encoding="utf-8") as f:
              json.dump({"fetched_at": fetched_at, "sections": sections}, f, ensure_ascii=False, indent=2)

          with open("latest_news.json", "w", encoding="utf-8") as f:
              json.dump({"fetched_at": fetched_at, "stories": sections.get("latest", [])}, f, ensure_ascii=False, indent=2)

          print("Wrote sections:", ", ".join(f"{k} ({len(v)} stories)" for k, v in sections.items()))
          PY

      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add news_sections.json latest_news.json
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi
          git commit -m "chore: update news sections"
          git push
